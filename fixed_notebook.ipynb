{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenfan20/AI-Agents-using-LangChain/blob/main/Vietnamese_Legal_Traffic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If8uKmFJb7WP"
      },
      "source": [
        "# T\u1ed5ng quan d\u1ef1 \u00e1n \ud83e\udd16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2YLtB01cSR-"
      },
      "source": [
        "Notebook n\u00e0y tr\u00ecnh b\u00e0y c\u00e1ch t\u1ea1o m\u1ed9t chatbot b\u1eb1ng h\u1ec7 th\u1ed1ng RAG d\u1ef1a v\u00e0o th\u01b0 vi\u1ec7n LangChain v\u00e0 LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6fB8o1BHxf1",
        "outputId": "42dec7b9-f274-4f0c-afca-22dc6d7f50fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: fsspec 2025.3.2\n",
            "Uninstalling fsspec-2025.3.2:\n",
            "  Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gcsfs 2025.3.2\n",
            "Uninstalling gcsfs-2025.3.2:\n",
            "  Successfully uninstalled gcsfs-2025.3.2\n",
            "Collecting fsspec==2024.12.0\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets==3.5.0\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting gcsfs==2024.12.0\n",
            "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.5.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (4.67.1)\n",
            "Collecting xxhash (from datasets==3.5.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets==3.5.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (6.0.2)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.19.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.20.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2025.4.26)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs==2024.12.0) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.12.0) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.5.0) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.12.0) (3.2.2)\n",
            "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, gcsfs\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 gcsfs-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y fsspec datasets gcsfs\n",
        "!pip install fsspec==2024.12.0 datasets==3.5.0 gcsfs==2024.12.0\n",
        "!pip install -q torch transformers accelerate bitsandbytes \\\n",
        "  langchain sentence-transformers faiss-cpu openpyxl pacmap datasets \\\n",
        "  langchain-community ragatouille tqdm pymupdf python-docx pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDm7mOE_H7Xh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "FILE_01 = '/content/luatgt.pdf'\n",
        "\n",
        "\n",
        "VECTOR_DATABASE_PATH = '/content/vectordatabase'\n",
        "os.makedirs('/content/vectordatabase', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbi5bd0sjjf"
      },
      "source": [
        "# Chunking \ud83d\udd2a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1B1p27JLJ1P"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from docx import Document as DocxDocument\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_pdf_file(file_path):\n",
        "    \"\"\"Loads a PDF file and returns its entire content using PyMuPDFLoader.\"\"\"\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    full_content = \"\"\n",
        "    for doc in documents:\n",
        "        full_content += doc.page_content + \"\\n\"  # Add a newline to separate pages\n",
        "    return full_content  # Return the entire content\n",
        "\n",
        "def load_txt_file(file_path):\n",
        "    \"\"\"Loads a TXT file and returns its content as a string.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def load_docx_file(file_path):\n",
        "    \"\"\"Loads a DOCX file and returns its content as a string.\"\"\"\n",
        "    doc = DocxDocument(file_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return \"\\n\".join(full_text)\n",
        "\n",
        "def load_csv_file(file_path):\n",
        "    \"\"\"Loads a CSV file and concatenates all rows as a single string.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.to_string(index=False)  # Converts the DataFrame to a string (without row indices)\n",
        "\n",
        "def load_file(file_path):\n",
        "    \"\"\"Determines the file type and loads the file content.\"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == '.txt':\n",
        "        return load_txt_file(file_path)\n",
        "    elif ext in ['.doc', '.docx']:\n",
        "        return load_docx_file(file_path)\n",
        "    elif ext == '.pdf':\n",
        "        return load_pdf_file(file_path)\n",
        "    elif ext == '.csv':\n",
        "        return load_csv_file(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzuAON3ELhlo",
        "outputId": "3aab8579-05a6-41b4-99dd-b7efbc7dbb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.19it/s]\n"
          ]
        }
      ],
      "source": [
        "# List of document file paths (PDF, DOCX, TXT, CSV)\n",
        "file_paths = [FILE_01] # Example file paths. It can be like: file_paths = [FILE_01, FILE_02,..]\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = []\n",
        "for file_path in tqdm(file_paths):\n",
        "    try:\n",
        "        content = load_file(file_path)\n",
        "        RAW_KNOWLEDGE_BASE.append(\n",
        "            LangchainDocument(page_content=content, metadata={\"source\": file_path})\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mOxisvoLnZP",
        "outputId": "a8dd485b-694b-43dc-bd26-b1fff0064d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: /content/luatgt2.pdf\n",
            "Content snippet: CH\u00cdNH PH\u1ee6\n",
            "--------\n",
            "C\u1ed8NG H\u00d2A X\u00c3 H\u1ed8I CH\u1ee6NGH\u0128A VI\u1ec6T NAM\n",
            "\u0110\u1ed9c l\u1eadp - T\u1ef1do - H\u1ea1nh ph\u00fac\n",
            "---------------\n",
            "S\u1ed1: 168/2024/N\u0110-CP\n",
            "H\u00e0 N\u1ed9i, ng\u00e0y 26 th\u00e1ng 12 n\u0103m 2024\n",
            "NGH\u1eca\u0110\u1ecaNH\n",
            "QUY \u0110\u1ecaNH X\u1eecPH\u1ea0T VI PH\u1ea0M H\u00c0NH CH\u00cdNH V\u1ec0TR\u1eacT T\u1ef0, AN TO\u00c0N GIAO\n",
            "TH\u00d4NG TRONG L\u0128NH V\u1ef0C GIAO TH\u00d4NG \u0110\u01af\u1edcNG B\u1ed8; TR\u1eea\u0110I\u1ec2M, PH\u1ee4C H\u1ed2I\n",
            "\u0110I\u1ec2M GI\u1ea4Y PH\u00c9P L\u00c1I XE\n",
            "C\u0103n c\u1ee9Lu\u1eadt T\u1ed5ch\u1ee9c Ch\u00ednh ph\u1ee7ng\u00e0y 19 th\u00e1ng 6 n\u0103m 2015; Lu\u1eadt s\u1eeda \u0111\u1ed5i, b\u1ed5sung m\u1ed9t\n",
            "s\u1ed1\u0111i\u1ec1u c\u1ee7a Lu\u1eadt T\u1ed5ch\u1ee9c Ch\u00ednh ph\u1ee7v\u00e0 Lu\u1eadt T\u1ed5ch\u1ee9c ch\u00ednh quy\u1ec1n \u0111\u1ecba ph\u01b0\u01a1ng ng\u00e0y 22\n",
            "th\u00e1ng 11 n\u0103m 2019;\n",
            "C\u0103n c\u1ee9Lu\u1eadt X\u1eedl\u00fd vi ph\u1ea1m h\u00e0...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    print(f\"Source: {doc.metadata['source']}\")\n",
        "    print(f\"Content snippet: {doc.page_content[:500]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU34rNVHLrDP",
        "outputId": "6e508d87-cda0-4e8a-cc68-f02d1e1b32d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Ch\u01b0\u01a1ng I\n",
            "NH\u1eeeNG QUY \u0110\u1ecaNH CHUNG\n",
            "\u0110i\u1ec1u 1. Ph\u1ea1m vi \u0111i\u1ec1u ch\u1ec9nh\n",
            "1. Ngh\u1ecb\u0111\u1ecbnh n\u00e0y quy \u0111\u1ecbnh v\u1ec1:\n",
            "a) X\u1eedph\u1ea1t vi ph\u1ea1m h\u00e0nh ch\u00ednh v\u1ec1tr\u1eadt t\u1ef1, an to\u00e0n giao th\u00f4ng trong l\u0129nh v\u1ef1c giao th\u00f4ng\n",
            "\u0111\u01b0\u1eddng b\u1ed9bao g\u1ed3m: h\u00e0nh vi vi ph\u1ea1m h\u00e0nh ch\u00ednh; h\u00ecnh th\u1ee9c, m\u1ee9c x\u1eedph\u1ea1t, bi\u1ec7n ph\u00e1p kh\u1eafc\n",
            "ph\u1ee5c h\u1eadu qu\u1ea3\u0111\u1ed1i v\u1edbi t\u1eebng h\u00e0nh vi vi ph\u1ea1m h\u00e0nh ch\u00ednh; th\u1ea9m quy\u1ec1n l\u1eadp bi\u00ean b\u1ea3n, th\u1ea9m\n",
            "quy\u1ec1n x\u1eedph\u1ea1t, m\u1ee9c ph\u1ea1t ti\u1ec1n c\u1ee5th\u1ec3theo t\u1eebng ch\u1ee9c danh \u0111\u1ed1i v\u1edbi h\u00e0nh vi vi ph\u1ea1m h\u00e0nh\n",
            "ch\u00ednh v\u1ec1tr\u1eadt t\u1ef1, an to\u00e0n giao th\u00f4ng trong l\u0129nh v\u1ef1c giao th\u00f4ng \u0111\u01b0\u1eddng b\u1ed9;\n",
            "b) M\u1ee9c tr\u1eeb\u0111i\u1ec3m gi\u1ea5y ph\u00e9p l\u00e1i xe \u0111\u1ed1i v\u1edbi t\u1eebng h\u00e0nh vi vi ph\u1ea1m h\u00e0nh ch\u00ednh; tr\u00ecnh t\u1ef1, th\u1ee7\n",
            "t\u1ee5c, th\u1ea9m quy\u1ec1n tr\u1eeb\u0111i\u1ec3m, ph\u1ee5c h\u1ed3i \u0111i\u1ec3m gi\u1ea5y ph\u00e9p l\u00e1i xe \u0111\u1ec3qu\u1ea3n l\u00fd vi\u1ec7c ch\u1ea5p h\u00e0nh ph\u00e1p\n",
            "lu\u1eadt v\u1ec1tr\u1eadt t\u1ef1, an to\u00e0n giao th\u00f4ng \u0111\u01b0\u1eddng b\u1ed9c\u1ee7a ng\u01b0\u1eddi l\u00e1i xe.\n",
            "2. C\u00e1c h\u00e0nh vi vi ph\u1ea1m h\u00e0nh ch\u00ednh trong l\u0129nh v\u1ef1c qu\u1ea3n l\u00fd nh\u00e0 n\u01b0\u1edbc kh\u00e1c li\u00ean quan \u0111\u1ebfn\n",
            "tr\u1eadt t\u1ef1, an to\u00e0n giao th\u00f4ng trong l\u0129nh v\u1ef1c giao th\u00f4ng \u0111\u01b0\u1eddng b\u1ed9m\u00e0 kh\u00f4ng quy \u0111\u1ecbnh t\u1ea1i\n",
            "Ngh\u1ecb\u0111\u1ecbnh n\u00e0y th\u00ec \u00e1p d\u1ee5ng quy \u0111\u1ecbnh t\u1ea1i c\u00e1c Ngh\u1ecb\u0111\u1ecbnh quy \u0111\u1ecbnh v\u1ec1x\u1eedph\u1ea1t vi ph\u1ea1m h\u00e0nh' metadata={'source': '/content/luatgt2.pdf', 'start_index': 904}\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "    chunk_overlap=100,  # The number of characters to overlap between chunks\n",
        "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])\n",
        "print(docs_processed[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXsfngHhuDnE"
      },
      "source": [
        "# Embedding document \ud83d\udcc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "a4520d61e51f4400aa7936d6a186dbe7",
            "d344d507f41f4f4591b5ac6cbdedd93f",
            "a13b3e3956244ee188e24bf754d02b7b",
            "af0764699a7247d59e0480f570b5e754",
            "bb61d3b342b44506a4b4630ca6d64a1e",
            "5db6c290623945e6ae4077dabd77fd56",
            "8ab0f661b9154b95b03db2e8563d0296",
            "b2dfc854f9f046c0a095eda5cf12443b",
            "410101c9c03446f58f92f1d231f1fd64",
            "966cb226bc8d4c6d85afe833892a2415",
            "921083fd1d8c46e988487d82f9f112c9",
            "6fb6ab98f957471cb437a4d9dbcca807",
            "1fc566f171794ac488fc8b25bc93f8f5",
            "84e5520ebfc5477ea00f1928aa36b781",
            "6b3f92eb53714ca79b45ee674c337bed",
            "7a4eaa3b161445239b35a130316670f8",
            "e973508a730d4bd495e3b747e0116399",
            "63b34fbc8ceb49f2a29748ec8cf7e84b",
            "d28b50daa1534a92b4b5f22a2354ac62",
            "83de02611e0c4a0194f1f188886f727d"
          ]
        },
        "id": "GPG-aM8L6pe5",
        "outputId": "e66d368c-7217-4a56-b2dd-33ba6742a8b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4520d61e51f4400aa7936d6a186dbe7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "5122dd1bf2ea42c1be649f2a910fc854",
            "52d9fb509bdd40d2a3d604f7593961f1",
            "f08e61ea5e9c4e708aa2063356b0d22b",
            "0072b9be59344bb690b892d480f56d33",
            "527cc5f464504f839e325833f2f84447",
            "575c62885b7d4c828fb30935437f43ca",
            "722b041ea388444980e63fb1ca9ff8a6",
            "b724923fa04141a2bb4066e6dc98af50",
            "dd27d4bea3ca4cd5af5fa2addab687e0",
            "932f5841e29f4db48ccff26884b61958",
            "a94304c5a4534f3a9e2ccc3ecdfecc4d",
            "58b0f46950e744ccabd02d7e5551c037",
            "edf1f77cc42144ebaf0150785093279e",
            "e90386994f404a74bb0970a3769f3471",
            "c7667a16bd794e75ba16c18f9a387086",
            "a950ae3a8cb2441ba897b56cf4e420dd",
            "36b1a864285b460784fbbd2700268b86",
            "1203a6ce0c334b7a923a91e28545f581",
            "93c8988edc304c678c150e100ed05160",
            "24c81002ee9e47b79a1111598abd7abe",
            "628466cd4ce046e8b305182cdd10c8e2",
            "5474f40aa4c544deb887a739c0f0bdd1",
            "9460d4eddd4040a086298663c3c9ed44",
            "4eaf5c0f5be04ea28b45676676db5d97",
            "d5aca3184d2f4471b27992dc622fca1e",
            "8a8a26058b864e60aa7f62d01b839d46",
            "cca4c50ee3e943559e8e44ec1f28c386",
            "aeb87e06d4494f288ad0ad9262d641c5",
            "6f588d3848b245948a666d9a304e99f6",
            "7d82fe0f849a48568dbfbb32ca2d1386",
            "64df330b19da4825be7d07f82e5aefc7",
            "a417745162a6426ea2134e26f21c9c2a",
            "7f2d0b4fc9694985a03ee393bf247be2",
            "f40d13a9ec3f4d0da13d96da362f22a3",
            "22711488e69f4d848e3321cfc0dcd2e3",
            "2daf50fd8f274bc192bee5e437b4648d",
            "42df3f9ba0db4b9c828c6d7df9f1d69e",
            "73046765553a49f1a4bebc1366c378f2",
            "b31da6a498604333a7385f21e93d209c",
            "ff158e6260b944f0a4109a86acc7c78d",
            "5549661e89f34619bb876a80a004abaf",
            "7384668b933e43ef94c4c0128cb21f1b",
            "927b50a23c2841019a7adbaa0f56ee29",
            "397fdc5f056540d5afaf9b0b586b5952"
          ]
        },
        "id": "h9SqYZjLL1fl",
        "outputId": "7e7ea8d7-32f6-458d-b1e4-25428e1ceb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5122dd1bf2ea42c1be649f2a910fc854"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58b0f46950e744ccabd02d7e5551c037"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9460d4eddd4040a086298663c3c9ed44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f40d13a9ec3f4d0da13d96da362f22a3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"AITeamVN/Vietnamese_Embedding\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    258,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ8b7xebvXrz"
      },
      "source": [
        "# Vector Database \ud83d\udcc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "aedc859727cf4613911ffa2327b10e86",
            "88a7130c12a543608d827ede0dfec23c",
            "1d9e013d69454a988c560bf24d5e60b2",
            "20e6b43b4b4d45d883a1602929537ab7",
            "47c230630d864b6d8e67013fc44b2096",
            "954f13da29b34b62b2f4e9934a930e68",
            "dd717691d1f34447bfb24286c70452b2",
            "cf4807f1fb964f3fa55e347e761f01fc",
            "151715e08e784e97be176504b198a8b6",
            "e2edca1eb7374833a3848b933b783e39",
            "d4624719caa14a5aa22b63111aaa86b1",
            "ac5a97241cb24861939fdcf6295cd176",
            "4e3c8306fb3c43f0acf398c58d4e6639",
            "3e90c33e90464a3aadb2dfc31d0fc9d4",
            "d892e9cf3b3942d8b6e3718f7c209b8a",
            "3ae36a888d014c11b62e7c10ab17365c",
            "f0e3f9b2bf2a4db097843655e0adfec2",
            "3843e9343ec54f43a51c6057965394eb",
            "8a4ae81708d84561b9331029c8e118df",
            "0cca4279feb245ebbf60c3210b667c4e",
            "0037f9f3e8ee410093bc86e36840d6be",
            "3fecd0480bbb46a9b64e64dd78a6fc7d",
            "2982d383e1c840a0a9ea81f0f148697f",
            "145b4dbd603240eb8921a355f67ba6a8",
            "52f400e1cb56467db8bad849e01ac2cd",
            "9d2b2110a1f94140a6a945bee9654736",
            "37ec053537ef451391de461244d40e58",
            "577ebf96b6cc4c4699ac44c0ea5e15c7",
            "4af69c19156d444eb1b8327d03fefef8",
            "3d570b141ce34af2996253c5d11ad042",
            "12d02eab98d14ac6b5f791889938b5af",
            "e280f8908a4240f08216a4658a502d99",
            "0ac6c281d99149ffabc50a84392d4182",
            "4d6e8432b03f455194150bff6262ec32",
            "c48053d1e1b44ffd8d59e4e8aea85376",
            "ac556d00db424666be502ef0f8fc36ee",
            "33fce8cb3f7043f8a6e639102ed0ef37",
            "8c072a2870b24a1c8965517cda0ff947",
            "b1b98f8bc2db433a8cc1c4b9e6aa3c11",
            "35df1a9fe03446ada783d3cf2cea6a13",
            "9b0beb6fa9304f4e84dbaab75eed06bb",
            "aca5540e69b14afcaba6da6bec70fb15",
            "e1cad2decbef40c1b63efc86560ae26e",
            "bca4a69c0c4b4b90a4d8b79d7b1db294",
            "ea33fbcc31ea4325a7c7602bba916e44",
            "30b0cbb7637a468eacd6a07e7b360659",
            "339218da925746929180b88940d6f1d8",
            "85b779d700ac4e9ab487ede7d4c21bfe",
            "989a37d6bd0044e4947ed0edae4609f3",
            "d0b3804163a540afaa8d21b4fb71781c",
            "bb859c085e6e4755a491168360450768",
            "16e6ff196daa4d5ba71e8afdd512e5c4",
            "f55531c67ead4a1dbd12cd0751be14c2",
            "a482e2152b474a0d8841ac641aeb4c6d",
            "715f0fbbeb1d43579f0c2b44b49c14ae",
            "61326ecf28a14e16814000dcaa8978df",
            "fa2468773dc144a4978a83f1440c00a9",
            "c3cc93472b7547569f9f47da301190f6",
            "817fb302efca49b28c20450fd8f51028",
            "90e7d2b15e6a48fb91b22fcdb7003fc1",
            "a6356b145ac248b2b522a0c41683a470",
            "4136abccdde04e96ae43d0b6c6482bf1",
            "770017db726548698a2f4e7bef982318",
            "625d7f7a8b944a7880993e92378f0540",
            "dc671de7373f47ffa90127c1d9ac1ca3",
            "6a3cd38d2dae4540b13923105b303edf",
            "ead0712794b64663a2b832c76700b5b6",
            "f0647ce56ca24ea49c7ca581bdd4584e",
            "db5c42f642d44415a865769963231c39",
            "36755ef31ff9474db2da6f7340ec272b",
            "0929c84431c14945a0a5f91602a41b6a",
            "1b24d295ed474c6fa37577c93d09ff70",
            "308411d4b0544e4ab250f4a5eff7c570",
            "271d18780add46b7b1a477da82054ca5",
            "851716d1d92c450683adadb697e1f44a",
            "197363fd1c734503a8d05b6928c54607",
            "b2a08f73812c41c992c457378372339f"
          ]
        },
        "id": "D2sMpEQtMKzL",
        "outputId": "f80eb921-b638-489f-b1f7-ebdbea0d7ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a460f51192b9>:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aedc859727cf4613911ffa2327b10e86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5a97241cb24861939fdcf6295cd176"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/3.51k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2982d383e1c840a0a9ea81f0f148697f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d6e8432b03f455194150bff6262ec32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea33fbcc31ea4325a7c7602bba916e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61326ecf28a14e16814000dcaa8978df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ead0712794b64663a2b832c76700b5b6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGgwbRvwgeJ"
      },
      "source": [
        "# L\u01b0u Vector Database \u0111\u01b0\u1ee3c x\u1eed l\u00fd v\u00e0o c\u00e1c th\u01b0 m\u1ee5c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehKDq37DMfrt",
        "outputId": "2963ee0a-df24-4427-8d50-ccc43e7a0c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index saved to /content/vectordatabase/faiss_index.bin\n",
            "Document store saved to /content/vectordatabase/docstore.pkl\n",
            "Index to document store ID mapping saved to /content/vectordatabase/index_to_docstore_id.pkl\n",
            "Vector database loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import faiss  # Make sure to import FAISS\n",
        "import pickle\n",
        "\n",
        "# Save the FAISS index to a file\n",
        "faiss_file_path = os.path.join(VECTOR_DATABASE_PATH, 'faiss_index.bin')\n",
        "faiss.write_index(KNOWLEDGE_VECTOR_DATABASE.index, faiss_file_path)\n",
        "print(f\"FAISS index saved to {faiss_file_path}\")\n",
        "\n",
        "# Save the document store to a pickle file\n",
        "docstore_file_path = os.path.join(VECTOR_DATABASE_PATH, 'docstore.pkl')\n",
        "with open(docstore_file_path, 'wb') as f:\n",
        "    pickle.dump(KNOWLEDGE_VECTOR_DATABASE.docstore, f)\n",
        "print(f\"Document store saved to {docstore_file_path}\")\n",
        "\n",
        "# Save the index_to_docstore_id mapping\n",
        "mapping_file_path = os.path.join(VECTOR_DATABASE_PATH, 'index_to_docstore_id.pkl')\n",
        "with open(mapping_file_path, 'wb') as f:\n",
        "    pickle.dump(KNOWLEDGE_VECTOR_DATABASE.index_to_docstore_id, f)\n",
        "print(f\"Index to document store ID mapping saved to {mapping_file_path}\")\n",
        "\n",
        "# Load the FAISS index from the file\n",
        "loaded_index = faiss.read_index(faiss_file_path)\n",
        "\n",
        "# Load the document store from the pickle file\n",
        "with open(docstore_file_path, 'rb') as f:\n",
        "    loaded_docstore = pickle.load(f)\n",
        "\n",
        "# Load the index_to_docstore_id mapping\n",
        "with open(mapping_file_path, 'rb') as f:\n",
        "    index_to_docstore_id = pickle.load(f)\n",
        "\n",
        "# Create the FAISS vector store using the loaded index and document store\n",
        "loaded_vector_database = FAISS(\n",
        "    index=loaded_index,\n",
        "    docstore=loaded_docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id,\n",
        "    embedding_function=embedding_model.embed_query\n",
        ")\n",
        "print(\"Vector database loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcRAMGk7wwjK"
      },
      "source": [
        "# Chain v\u1edbi th\u01b0 m\u1ee5c \u0111\u00e3 l\u01b0u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x72vDcblMr4w"
      },
      "outputs": [],
      "source": [
        "# Embed a user query in the same space\n",
        "user_query = \"Quy \u0111\u1ecbnh v\u1ec1 \u0111\u1ed9 tu\u1ed5i l\u00e1i xe m\u00f4 t\u00f4 hai b\u00e1nh?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tVUVC5_M6B6",
        "outputId": "9c155f42-eb30-4d3b-8d99-a0da32c1a020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting retrieval for user_query='Quy \u0111\u1ecbnh v\u1ec1 \u0111\u1ed9 tu\u1ed5i l\u00e1i xe m\u00f4 t\u00f4 hai b\u00e1nh?'...\n",
            "\n",
            "==================================Top document==================================\n",
            "Giao th\u00f4ng \u0111\u01b0\u1eddng b\u1ed9n\u0103m 1968 c\u1ea5p (tr\u1eebgi\u1ea5y ph\u00e9p l\u00e1i xe qu\u1ed1c t\u1ebfdo Vi\u1ec7t Nam c\u1ea5p)\n",
            "nh\u01b0ng kh\u00f4ng mang theo gi\u1ea5y ph\u00e9p l\u00e1i xe qu\u1ed1c gia ph\u00f9 h\u1ee3p v\u1edbi lo\u1ea1i xe \u0111\u01b0\u1ee3c ph\u00e9p \u0111i\u1ec1u\n",
            "khi\u1ec3n;\n",
            "c) S\u1eedd\u1ee5ng gi\u1ea5y ph\u00e9p l\u00e1i xe kh\u00f4ng h\u1ee3p l\u1ec7(gi\u1ea5y ph\u00e9p l\u00e1i xe c\u00f3 s\u1ed1ph\u00f4i ghi \u1edfm\u1eb7t sau\n",
            "kh\u00f4ng tr\u00f9ng v\u1edbi s\u1ed1ph\u00f4i \u0111\u01b0\u1ee3c c\u1ea5p m\u1edbi nh\u1ea5t trong h\u1ec7th\u1ed1ng th\u00f4ng tin qu\u1ea3n l\u00fd gi\u1ea5y ph\u00e9p l\u00e1i\n",
            "xe).\n",
            "6. Ph\u1ea1t ti\u1ec1n t\u1eeb4.000.000 \u0111\u1ed3ng \u0111\u1ebfn 6.000.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi t\u1eeb\u0111\u1ee716 tu\u1ed5i \u0111\u1ebfn d\u01b0\u1edbi\n",
            "18 tu\u1ed5i \u0111i\u1ec1u khi\u1ec3n xe \u00f4 t\u00f4, xe ch\u1edfng\u01b0\u1eddi b\u1ed1n b\u00e1nh c\u00f3 g\u1eafn \u0111\u1ed9ng c\u01a1, xe ch\u1edfh\u00e0ng b\u1ed1n b\u00e1nh\n",
            "c\u00f3 g\u1eafn \u0111\u1ed9ng c\u01a1 v\u00e0 c\u00e1c lo\u1ea1i xe t\u01b0\u01a1ng t\u1ef1xe \u00f4 t\u00f4.\n",
            "7. Ph\u1ea1t ti\u1ec1n t\u1eeb6.000.000 \u0111\u1ed3ng \u0111\u1ebfn 8.000.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe m\u00f4 t\u00f4 hai\n",
            "b\u00e1nh c\u00f3 dung t\u00edch xi-lanh tr\u00ean 125 cm3 tr\u1edfl\u00ean ho\u1eb7c c\u00f3 c\u00f4ng su\u1ea5t \u0111\u1ed9ng c\u01a1 \u0111i\u1ec7n tr\u00ean 11\n",
            "kW, xe m\u00f4 t\u00f4 ba b\u00e1nh th\u1ef1c hi\u1ec7n m\u1ed9t trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) C\u00f3 gi\u1ea5y ph\u00e9p l\u00e1i xe nh\u01b0ng kh\u00f4ng ph\u00f9 h\u1ee3p v\u1edbi lo\u1ea1i xe \u0111ang \u0111i\u1ec1u khi\u1ec3n;\n",
            "==================================Metadata==================================\n",
            "{'source': '/content/luatgt2.pdf', 'start_index': 99502}\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "# retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "\n",
        "retrieved_docs = loaded_vector_database.similarity_search(query=user_query, k=5)\n",
        "\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEvU27LIxcTM"
      },
      "source": [
        "# \u0110\u01b0a LLM \u0111\u1ec3 t\u1ea1o sinh c\u00e2u tr\u1ea3 l\u1eddi v\u1edbi d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "7b9a6150fd9546eeb4c24b8ac7b425e8",
            "323fb8c435ed4cb8ba70834592c67204",
            "299fc6fe4b6e4c76bf8752342490766d",
            "6aebbd4b3baa4308b1a8b90b5cff0793",
            "4311a63aa0494c879f06a8b8d94a852c",
            "93b45de04a364f84b351690854f93261",
            "2f357822bc4c46d1aa19c840c48312a1",
            "f201d78345594b75a2a1f5c78d63006f",
            "7d5e04ae00e74c4ba3f5c05104a75aa9",
            "b2f6d8fc2f8f4ab5a11aa26a22c6c443",
            "72c20a9a570f403e9d2b6cba3c8d8d2c",
            "6dae2206950e42e3a0da7c1b671a018f",
            "a2bca78becf64d92af1f6b335f3b9f0a",
            "11c65dc4bbfe403a9ca3e9ccb2b1a001",
            "cc228e9819ca499ea5036742b9b65c58",
            "d02cce1ae85a41788ce4a09bed0049da",
            "56752ea52a154d40904f8adae95cb0fe",
            "503bfa66d54b4809a818dc83fcca74f4",
            "a50ef2610280497fb669c10466c7938a",
            "e7402a939a30489682fa55e3531118a3",
            "30fc32bbe0cd4ef0a42f179f66e7e89a",
            "0de043d9bf9c42c8b7ecbb4ce91a8901",
            "d84e835fa36b46ae9a1ae1008fe8afe7",
            "1ccbd36dd71b443e8ec6bc36065ea11d",
            "508d5a999d114d68b6458de4fddcbcba",
            "20b9bf4156d34eba9afe4cd281cc7c4a",
            "b8b932bfe77a471eb87452cb8d810c4d",
            "3fc15f755b3c4bb984ed82b550fd652c",
            "ea921fb84aa64850969595be33e01c0e",
            "fc84805691cd425aae391a1c93eb5722",
            "e9781deabbcc4d879c64c48b12c0906e",
            "f490309f081d468f934db1a8f9431840",
            "6fbdbb07cc1b480ba242e2a1c44a39b8",
            "d079d4976657487b849c02a259d67e2b",
            "03d1e4615f1c41e79a894ca9489fae66",
            "4ebc3d950c1e4d65844aefb8715f5888",
            "bd77620e45b34fa59d4ba9e82f62ef51",
            "22889dec51ce416a8e470c770de14477",
            "0de787478d3449bf9b4c2fc5c96fe308",
            "bc0c23f0bf57449b9985807f8faa31c0",
            "66259fbb2b52450281c60dadfd4f8be1",
            "f908344601c744e284351fdbfcc7d120",
            "bd2b00bb37894c19b60bc000a2da5c00",
            "83a4a2e5b29846c1881cf6d118ed7e7b",
            "3ff191aa2c6f4f2aab54d295f561f4a1",
            "d62a060b0a5c44e9a85cb20bbfb4aac6",
            "0aacba10988f4b5d914b7807394bf30e",
            "9167eebc803749e5a2f8ad438360534f",
            "e5400a93978e4ab3bfcb703fc502aa93",
            "b8678f3a48884c49a271c4dfa60835dd",
            "47b4b2a59e534861a1923b07d04c9033",
            "15ccc0fd0f754290bc203c8a64e4edf0",
            "e972a77ae38f42fc96030088e7259b49",
            "4f293367c277439e8f020de59f381f7a",
            "5cded3d5ac7f403596aedb09490fcd12",
            "f6a7f31af85a40d2b05706a1266ed294",
            "b1cf25cf15a84a4dabe3c1f35e14ef92",
            "d389a0e2c68b40988581ca6889b9ea5b",
            "485e8a7df71945ca843b5b6e31748c15",
            "2cc695f4de0544839b0fac0ce8c9ae88",
            "464e28c188c749d998c1b79ee64495e3",
            "453026e69d4944178368f27e1554edb1",
            "e9081a16c86a4981ba1ca40035297c0b",
            "3f50f99a34e84bf79f21505bb58668d7",
            "4f20b036cde54bb0bc60ecd81dc6889c",
            "b2ee66648b3044dcb89ccb95609565f8",
            "096e35fd80ab4fadac4229c8ee96387e",
            "400ccf3b8ac34edc8b1455c882a9a4f2",
            "c29a4c6d1e8842da9b49914da90e4e65",
            "31cc62415ea74875bb258e535facd7ab",
            "6e3a6340a1e54cb48f069d2ed59aa88d",
            "84a3ed87b0f54c54b058816baee8791e",
            "3cdcf6aba41d499fad7ad84a9326fe43",
            "97856cc232e14934a35f16b8e82afacd",
            "cabf82cb9b4043968c15f8c6583da612",
            "29d3e6a9cefd4b91b6f517e777ca53db",
            "0b20a711ffc14043ac40be475a52feb5"
          ]
        },
        "id": "ZIju0plfNAjW",
        "outputId": "80cc0911-d86e-421e-989c-a12bfa6b6cc2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b9a6150fd9546eeb4c24b8ac7b425e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dae2206950e42e3a0da7c1b671a018f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d84e835fa36b46ae9a1ae1008fe8afe7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d079d4976657487b849c02a259d67e2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff191aa2c6f4f2aab54d295f561f4a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6a7f31af85a40d2b05706a1266ed294"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "096e35fd80ab4fadac4229c8ee96387e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# C\u1ea5u h\u00ecnh bitsandbytes \u0111\u1ec3 t\u1ea3i m\u00f4 h\u00ecnh \u1edf \u0111\u1ecbnh d\u1ea1ng 4-bit ti\u1ebft ki\u1ec7m RAM\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # \u0110\u1ed5i sang float16 n\u1ebfu bfloat16 kh\u00f4ng \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3\n",
        ")\n",
        "\n",
        "# Load m\u00f4 h\u00ecnh v\u1edbi c\u1ea5u h\u00ecnh nh\u1eb9, t\u1ef1 \u0111\u1ed9ng ph\u00e2n b\u1ed5 thi\u1ebft b\u1ecb (GPU/CPU)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "# T\u1ea1o pipeline \u0111\u1ec3 d\u00f9ng m\u00f4 h\u00ecnh nh\u01b0 1 RAG Reader\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "READER_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # \u2705 TinyLLaMA model\n",
        "\n",
        "# C\u1ea5u h\u00ecnh t\u1ea3i m\u00f4 h\u00ecnh 4-bit \u0111\u1ec3 ti\u1ebft ki\u1ec7m RAM (ph\u00f9 h\u1ee3p GPU 4GB)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # float16 t\u01b0\u01a1ng th\u00edch v\u1edbi nhi\u1ec1u GPU\n",
        ")\n",
        "\n",
        "# Load m\u00f4 h\u00ecnh v\u1edbi c\u1ea5u h\u00ecnh nh\u1eb9\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "# T\u1ea1o pipeline x\u1eed l\u00fd v\u0103n b\u1ea3n\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")\n",
        "\n",
        "# V\u00ed d\u1ee5 s\u1eed d\u1ee5ng\n",
        "prompt = \"H\u00e3y gi\u1edbi thi\u1ec7u v\u1ec1 Vi\u1ec7t Nam b\u1eb1ng ti\u1ebfng Vi\u1ec7t.\"\n",
        "output = READER_LLM(prompt)\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "1450b03b74054b46a09ec8b8a6a8488b",
            "d80801d749b145659045e46cae924c08",
            "e9a9147ad9f94a9286b6a62290e95bda",
            "c899c646f46949fdbd3ee2086c2e11f6",
            "8add2ba03333497d871341aafdedff7f",
            "7a98a106213747f39f68b37f01ce9b50",
            "e21036183f884f76902b96089a96d874",
            "738e86d02bed4feeada7e36b3f61d93e",
            "0099e897726a4ba980eb28f40bb5ff72",
            "6551db9fb6784ba0a93583ddb4946732",
            "f5a52cf1be2d479db2ae4a659d882821",
            "de1ab308e64e4a9abd553f47f128c352",
            "a135b44129ee44f4bbd84fc53675dbbf",
            "dbe14f7481e9414a94d3f3c61e6365ec",
            "df98f3305259485e8bb2e3383764e712",
            "895d320ebfb34eb2b2cccf5d2b0371a8",
            "b5a34ba5249f4fc5823893ec45775734",
            "a2aa8a5b5aff49e78485e10c65813bfe",
            "930a1b54a3034f3c9b1dd42409a343c7",
            "73ce4a85a79b4d0194ed2ac8c8b4c326",
            "53ab490dd3634e2f9f87d9e20e78036f",
            "b5cf9ed0d9f74772a01f295d5a914fc6",
            "52478b708ae54f70843ee674ea3b8030",
            "7fe0cce4017e4726a13bc356974ddb99",
            "de585cc582574935ad024cb16783b4ce",
            "37caf5dc92984ef38266f5d6e9ecf00d",
            "6fbde0064ecf4b299e4679bfe48b3a2c",
            "b857ad34a6294fe39db7270f077ffa14",
            "ec80334540b34a4a9d33ac2b5f9d493d",
            "fc62886fce4c4801add7978718993fd9",
            "9b4c938540b6471686c6b50af28d6568",
            "60244027bf15409f8ab9e8e3547d7ad1",
            "049cbf4325db491b89a46a8da5ec865b",
            "e5274e6e8b2c42aea1d77c77d77e80d4",
            "e105d1ee367b48078004f7bcbf97799f",
            "76a1b434a2c14c63b5918438c97c42b9",
            "7137fb9de3824147b69ea3c4526aaddf",
            "8c4c9d4165a24bca92bf0e8fa3b3db01",
            "ebd7c5fa077a445c9666728c516f5b6a",
            "2c3800d79e0e425796601330cb5d7fc3",
            "b1ec3674f1284028b18efde35ccaaae0",
            "552f1c3b75dc49eeae04a8f0194e6f0e",
            "8e728689469b483c92e02385dbd35e5f",
            "3bb292f98cd74564aed753ae75ab088b",
            "3a0817b4322541e198e93f05835aecee",
            "d936c8fe3865443891ddf1d6203c5674",
            "389ebdddb25b4e77b7a6827fc03dcbce",
            "2846ee6e223c48a28c09c4558493fb95",
            "8fdabe0d2b52497980448be5530df725",
            "458c0533e4a64167a5d3bb76578cd76e",
            "46e1fadc88a14a81a1fdf4ebb1c2cf31",
            "9c7b36eac62848e5a460431157c260c4",
            "ce29f86d6c654810b7ec6830455910cd",
            "e012ec7249fe4080b3474a51e074c56a",
            "046fe55cf88f4f02bf67d1e40fafafea",
            "c54d3733d3d1488dbc2153bc9c60832b",
            "9cab66d2144040c4aad91d6e1dfd71df",
            "f515c7044b5d411fbd487d71eac45e46",
            "fbc1875875a64653b9177b1ec6087784",
            "4e8c2cd7fb514787ab977c673b0287ed",
            "307c9fa469104d12838ccb118de12265",
            "51047fb24a5440c2862304c96e3d53b6",
            "d8461d9a941d4600907ee110d1fc4abe",
            "fce21a73613444eb92007fd0474f18d8",
            "4e4e36bc5a894c7da7555bc73466b39b",
            "3410a865953d47288d898080caf18a74",
            "cda3e73bdfe140a3b096f8b645eae951",
            "029c242e79844fb594294ae24d0fb1aa",
            "99e4b802926f48928fa237fce5be2776",
            "ce7b22700b2c409590d1c183f85e4218",
            "ed7579553bd24e80b1481a04ebe437fb",
            "333bd3cf8fde401a8bcce9f664b63a3c",
            "7751388828504490a4a8051c9cd7090b",
            "67713980660b4f689451e9b3c74ff4cd",
            "19f0db40751f4d909978e54d1abf2079",
            "aef0fe717f6e4e2faedc8388d9b09568",
            "2898217034344e8eaee0fc770b4acca1"
          ]
        },
        "id": "LQ9zzaHRlN1S",
        "outputId": "4b6b7444-2afc-446f-ffb7-b1faed16dd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1450b03b74054b46a09ec8b8a6a8488b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de1ab308e64e4a9abd553f47f128c352"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52478b708ae54f70843ee674ea3b8030"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5274e6e8b2c42aea1d77c77d77e80d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a0817b4322541e198e93f05835aecee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c54d3733d3d1488dbc2153bc9c60832b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cda3e73bdfe140a3b096f8b645eae951"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vi\u1ec7t Nam kh\u00f4ng ch\u1ec9 l\u00e0 m\u1ed9t n\u01a1i \u0111\u01b0\u1ee3c t\u00ecm ki\u1ebfm, c\u00f2n c\u00f3 nhi\u1ec1u ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "\u0110\u00e2y l\u00e0 m\u1ed9t s\u1ed1 ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "Nh\u1eefng ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "Nh\u1eefng ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "Nh\u1eefng ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "Nh\u1eefng ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "Nh\u1eefng ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103ng l\u1ef1c.\n",
            "Nh\u1eefng ng\u01b0\u1eddi d\u00f9ng \u0111ang h\u00e0nh tr\u00ecnh xem x\u00e9t v\u00e0 tham gia c\u00e1c c\u00f4ng vi\u1ec7c \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c n\u0103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8du24nceNRgB",
        "outputId": "07f93dc9-69bc-429c-8267-57203e418155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-XNwX-iOBcP",
        "outputId": "36b8a334-c218-4a7e-a14f-8d2de664a4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "S\u1eed d\u1ee5ng th\u00f4ng tin trong ng\u1eef c\u1ea3nh, h\u00e3y \u0111\u01b0a ra c\u00e2u tr\u1ea3 l\u1eddi \u0111\u1ea7y \u0111\u1ee7 cho c\u00e2u h\u1ecfi.\n",
            "Ch\u1ec9 tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c h\u1ecfi, c\u00e2u tr\u1ea3 l\u1eddi c\u1ea7n ng\u1eafn g\u1ecdn v\u00e0 ph\u00f9 h\u1ee3p v\u1edbi c\u00e2u h\u1ecfi.\n",
            "Cung c\u1ea5p s\u1ed1 c\u1ee7a t\u00e0i li\u1ec7u ngu\u1ed3n khi ph\u00f9 h\u1ee3p.\n",
            "N\u1ebfu c\u00e2u tr\u1ea3 l\u1eddi kh\u00f4ng th\u1ec3 suy ra t\u1eeb ng\u1eef c\u1ea3nh, kh\u00f4ng \u0111\u01b0a ra c\u00e2u tr\u1ea3 l\u1eddi.</s>\n",
            "<|user|>\n",
            "Ng\u1eef c\u1ea3nh:\n",
            "{context}\n",
            "---\n",
            "B\u00e2y gi\u1edd \u0111\u00e2y l\u00e0 c\u00e2u h\u1ecfi m\u00e0 b\u1ea1n c\u1ea7n tr\u1ea3 l\u1eddi.\n",
            "\n",
            "C\u00e2u h\u1ecfi: {question}</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect\n",
        "\n",
        "prompt_in_chat_format_en = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prompt template for Vietnamese\n",
        "prompt_in_chat_format_vi = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"S\u1eed d\u1ee5ng th\u00f4ng tin trong ng\u1eef c\u1ea3nh, h\u00e3y \u0111\u01b0a ra c\u00e2u tr\u1ea3 l\u1eddi \u0111\u1ea7y \u0111\u1ee7 cho c\u00e2u h\u1ecfi.\n",
        "Ch\u1ec9 tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c h\u1ecfi, c\u00e2u tr\u1ea3 l\u1eddi c\u1ea7n ng\u1eafn g\u1ecdn v\u00e0 ph\u00f9 h\u1ee3p v\u1edbi c\u00e2u h\u1ecfi.\n",
        "Cung c\u1ea5p s\u1ed1 c\u1ee7a t\u00e0i li\u1ec7u ngu\u1ed3n khi ph\u00f9 h\u1ee3p.\n",
        "N\u1ebfu c\u00e2u tr\u1ea3 l\u1eddi kh\u00f4ng th\u1ec3 suy ra t\u1eeb ng\u1eef c\u1ea3nh, kh\u00f4ng \u0111\u01b0a ra c\u00e2u tr\u1ea3 l\u1eddi.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Ng\u1eef c\u1ea3nh:\n",
        "{context}\n",
        "---\n",
        "B\u00e2y gi\u1edd \u0111\u00e2y l\u00e0 c\u00e2u h\u1ecfi m\u00e0 b\u1ea1n c\u1ea7n tr\u1ea3 l\u1eddi.\n",
        "\n",
        "C\u00e2u h\u1ecfi: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "def detect_language(query):\n",
        "    return detect(query)\n",
        "\n",
        "def create_prompt(question):\n",
        "\n",
        "    language = detect_language(question)\n",
        "\n",
        "    if language == 'vi':\n",
        "        RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "            prompt_in_chat_format_vi, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    else:\n",
        "        RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "            prompt_in_chat_format_en, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "    return RAG_PROMPT_TEMPLATE\n",
        "\n",
        "#test prompt if the question is Vietnamese\n",
        "\n",
        "test_prompt = create_prompt(\"H\u00e0 N\u1ed9i l\u00e0 th\u1ee7 \u0111\u00f4 c\u1ee7a n\u01b0\u1edbc n\u00e0o?\")\n",
        "print(test_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhkV9GeSOMxi",
        "outputId": "3e1be57d-89f7-4180-8b16-2f0422fc253e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C\u00e2u tr\u1ea3 l\u1eddi: Ng\u01b0\u1eddi t\u1eeb 16 tu\u1ed5i \u0111\u1ebfn d\u01b0\u1edbi 18 tu\u1ed5i \u0111\u01b0\u1ee3c ph\u00e9p l\u00e1i xe m\u00f4 t\u00f4 c\u00f3 dung t\u00edch xi-lanh tr\u00ean 50 cm3 ho\u1eb7c c\u00f3 c\u00f4ng su\u1ea5t \u0111\u1ed9ng c\u01a1 \u0111i\u1ec7n t\u1eeb 04 kW tr\u1edf l\u00ean. Tuy nhi\u00ean, ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe m\u00f4 t\u00f4 n\u00e0 c\u1ea7n mang theo gi\u1ea5y ph\u00e9p l\u00e1i xe ph\u00f9 h\u1ee3p v\u1edbi lo\u1ea1i xe \u0111ang \u0111i\u1ec1u khi\u1ec3n. N\u1ebfu kh\u00f4ng mang theo gi\u1ea5y ph\u00e9p l\u00e1i xe nh\u01b0ng v\u1eabn \u0111i\u1ec1u khi\u1ec3n xe m\u00f4 t\u00f4 n\u00e0, ng\u01b0\u1eddi s\u1ebd b\u1ecb ph\u1ea1t theo quy \u0111\u1ecbnh.\n"
          ]
        }
      ],
      "source": [
        "retrieved_docs_text = [\n",
        "    doc.page_content for doc in retrieved_docs\n",
        "]  # We only need the text of the documents\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join(\n",
        "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
        ")\n",
        "\n",
        "user_query=\"Quy \u0111\u1ecbnh v\u1ec1 \u0111\u1ed9 tu\u1ed5i l\u00e1i xe m\u00f4 t\u00f4 2 b\u00e1nh?\"\n",
        "\n",
        "rag_prompt = create_prompt(user_query)\n",
        "final_prompt = rag_prompt.format(\n",
        "    question=user_query, context=context\n",
        ")\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqoSJDAEORYK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, Pipeline\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "\n",
        "    if not relevant_docs:\n",
        "        raise ValueError(\"No relevant documents retrieved.\")\n",
        "\n",
        "    # Gi\u1eef b\u1ea3n g\u1ed1c \u0111\u1ec3 return\n",
        "    retrieved_docs = relevant_docs[:num_docs_final]  # gi\u1eef l\u1ea1i LangchainDocument g\u1ed1c\n",
        "\n",
        "    # L\u1ea5y n\u1ed9i dung text \u0111\u1ec3 \u0111\u01b0a v\u00e0o prompt\n",
        "    docs_text = [doc.page_content for doc in retrieved_docs]\n",
        "\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n{doc}\" for i, doc in enumerate(docs_text)]\n",
        "    )\n",
        "\n",
        "    RAG_PROMPT_TEMPLATE = create_prompt(question)\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HBvtBwSP-gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0037e951-2f1f-4c9d-d593-528a6bae1352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Retrieving documents...\n",
            "=> Generating answer...\n"
          ]
        }
      ],
      "source": [
        "question = \"V\u01b0\u1ee3t \u0111\u00e8n \u0111\u1ecf ph\u1ea1t bao nhi\u00eau ti\u1ec1n\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "    question, READER_LLM, loaded_vector_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaj6DxnGQi4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c33a85-98a9-44cf-c1cc-d10f54ac1041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================Answer==================================\n",
            "V\u01b0\u1ee3t \u0111\u00e8n \u0111\u1ecf ph\u1ea1t t\u1eeb 4.000.000 \u0111\u1ed3ng \u0111\u1ebfn 6.000.000 \u0111\u1ed3ng, theo ng\u1eef c\u1ea3nh cung c\u1ea5p.\n",
            "==================================Source docs==================================\n",
            "Document 0------------------------------------------------------------\n",
            "page_content='gi\u1eefkho\u1ea3ng c\u00e1ch theo quy \u0111\u1ecbnh c\u1ee7a bi\u1ec3n b\u00e1o hi\u1ec7u \u201cC\u1ef1ly t\u1ed1i thi\u1ec3u gi\u1eefa hai xe\u201d, tr\u1eebc\u00e1c\n",
            "h\u00e0nh vi vi ph\u1ea1m quy \u0111\u1ecbnh t\u1ea1i \u0111i\u1ec3m d kho\u1ea3n 5 \u0110i\u1ec1u n\u00e0y.\n",
            "5. Ph\u1ea1t ti\u1ec1n t\u1eeb4.000.000 \u0111\u1ed3ng \u0111\u1ebfn 6.000.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c hi\u1ec7n\n",
            "m\u1ed9t trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) V\u01b0\u1ee3t xe trong nh\u1eefng tr\u01b0\u1eddng h\u1ee3p kh\u00f4ng \u0111\u01b0\u1ee3c v\u01b0\u1ee3t, v\u01b0\u1ee3t xe t\u1ea1i \u0111o\u1ea1n \u0111\u01b0\u1eddng c\u00f3 bi\u1ec3n b\u00e1o\n",
            "hi\u1ec7u c\u00f3 n\u1ed9i dung c\u1ea5m v\u01b0\u1ee3t (\u0111\u1ed1i v\u1edbi lo\u1ea1i ph\u01b0\u01a1ng ti\u1ec7n \u0111ang \u0111i\u1ec1u khi\u1ec3n); kh\u00f4ng c\u00f3 t\u00edn hi\u1ec7u\n",
            "tr\u01b0\u1edbc khi v\u01b0\u1ee3t ho\u1eb7c c\u00f3 t\u00edn hi\u1ec7u v\u01b0\u1ee3t xe nh\u01b0ng kh\u00f4ng s\u1eedd\u1ee5ng trong su\u1ed1t qu\u00e1 tr\u00ecnh v\u01b0\u1ee3t\n",
            "xe; v\u01b0\u1ee3t b\u00ean ph\u1ea3i xe kh\u00e1c trong tr\u01b0\u1eddng h\u1ee3p kh\u00f4ng \u0111\u01b0\u1ee3c ph\u00e9p;\n",
            "b) \u0110i\u1ec1u khi\u1ec3n xe kh\u00f4ng \u0111i b\u00ean ph\u1ea3i theo chi\u1ec1u \u0111i c\u1ee7a m\u00ecnh; \u0111i kh\u00f4ng \u0111\u00fang ph\u1ea7n \u0111\u01b0\u1eddng\n",
            "ho\u1eb7c l\u00e0n \u0111\u01b0\u1eddng quy \u0111\u1ecbnh (l\u00e0n c\u00f9ng chi\u1ec1u ho\u1eb7c l\u00e0n ng\u01b0\u1ee3c chi\u1ec1u) tr\u1eebh\u00e0nh vi quy \u0111\u1ecbnh t\u1ea1i\n",
            "\u0111i\u1ec3m a kho\u1ea3n 4 \u0110i\u1ec1u n\u00e0y; \u0111i\u1ec1u khi\u1ec3n xe \u0111i qua d\u1ea3i ph\u00e2n c\u00e1ch c\u1ed1\u0111\u1ecbnh \u1edfgi\u1eefa hai ph\u1ea7n\n",
            "\u0111\u01b0\u1eddng xe ch\u1ea1y;\n",
            "c) Tr\u00e1nh xe \u0111i ng\u01b0\u1ee3c chi\u1ec1u kh\u00f4ng \u0111\u00fang quy \u0111\u1ecbnh (tr\u1eebh\u00e0nh vi vi ph\u1ea1m s\u1eedd\u1ee5ng \u0111\u00e8n chi\u1ebfu' metadata={'source': '/content/luatgt2.pdf', 'start_index': -1}\n",
            "Document 1------------------------------------------------------------\n",
            "page_content='c) Kh\u00f4ng ch\u1ea5p h\u00e0nh hi\u1ec7u l\u1ec7nh c\u1ee7a \u0111\u00e8n t\u00edn hi\u1ec7u giao th\u00f4ng;\n",
            "d) Kh\u00f4ng ch\u1ea5p h\u00e0nh hi\u1ec7u l\u1ec7nh, h\u01b0\u1edbng d\u1eabn c\u1ee7a ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n giao th\u00f4ng ho\u1eb7c ng\u01b0\u1eddi\n",
            "ki\u1ec3m so\u00e1t giao th\u00f4ng;\n",
            "\u0111) Kh\u00f4ng nh\u01b0\u1eddng \u0111\u01b0\u1eddng ho\u1eb7c g\u00e2y c\u1ea3n tr\u1edfxe \u0111\u01b0\u1ee3c quy\u1ec1n \u01b0u ti\u00ean \u0111ang ph\u00e1t t\u00edn hi\u1ec7u \u01b0u\n",
            "ti\u00ean \u0111i l\u00e0m nhi\u1ec7m v\u1ee5.\n",
            "8. Ph\u1ea1t ti\u1ec1n t\u1eeb6.000.000 \u0111\u1ed3ng \u0111\u1ebfn 8.000.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c hi\u1ec7n\n",
            "m\u1ed9t trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) \u0110i\u1ec1u khi\u1ec3n xe ch\u1ea1y qu\u00e1 t\u1ed1c \u0111\u1ed9quy \u0111\u1ecbnh tr\u00ean 20 km/h;\n",
            "b) \u0110i\u1ec1u khi\u1ec3n xe tr\u00ean \u0111\u01b0\u1eddng m\u00e0 trong m\u00e1u ho\u1eb7c h\u01a1i th\u1edfc\u00f3 n\u1ed3ng \u0111\u1ed9c\u1ed3n v\u01b0\u1ee3t qu\u00e1 50\n",
            "miligam \u0111\u1ebfn 80 miligam/100 milil\u00edt m\u00e1u ho\u1eb7c v\u01b0\u1ee3t qu\u00e1 0,25 miligam \u0111\u1ebfn 0,4 miligam/1\n",
            "l\u00edt kh\u00ed th\u1edf.\n",
            "9. Ph\u1ea1t ti\u1ec1n t\u1eeb8.000.000 \u0111\u1ed3ng \u0111\u1ebfn 10.000.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c\n",
            "hi\u1ec7n m\u1ed9t trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) \u0110i\u1ec1u khi\u1ec3n xe l\u1ea1ng l\u00e1ch, \u0111\u00e1nh v\u00f5ng tr\u00ean \u0111\u01b0\u1eddng b\u1ed9; s\u1eedd\u1ee5ng ch\u00e2n ch\u1ed1ng ho\u1eb7c v\u1eadt kh\u00e1c\n",
            "qu\u1ec7t xu\u1ed1ng \u0111\u01b0\u1eddng khi xe \u0111ang ch\u1ea1y;\n",
            "b) \u0110i\u1ec1u khi\u1ec3n xe th\u00e0nh nh\u00f3m t\u1eeb02 xe tr\u1edfl\u00ean ch\u1ea1y qu\u00e1 t\u1ed1c \u0111\u1ed9quy \u0111\u1ecbnh;' metadata={'source': '/content/luatgt2.pdf', 'start_index': 42855}\n",
            "Document 2------------------------------------------------------------\n",
            "page_content='\u0111\u01b0\u1eddng b\u1ed9\n",
            "1. Ph\u1ea1t ti\u1ec1n t\u1eeb400.000 \u0111\u1ed3ng \u0111\u1ebfn 600.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c hi\u1ec7n m\u1ed9t\n",
            "trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) Kh\u00f4ng ch\u1ea5p h\u00e0nh hi\u1ec7u l\u1ec7nh, ch\u1ec9d\u1eabn c\u1ee7a bi\u1ec3n b\u00e1o hi\u1ec7u, v\u1ea1ch k\u1ebb\u0111\u01b0\u1eddng, tr\u1eebc\u00e1c h\u00e0nh vi\n",
            "vi ph\u1ea1m quy \u0111\u1ecbnh t\u1ea1i \u0111i\u1ec3m a, \u0111i\u1ec3m b, \u0111i\u1ec3m c, \u0111i\u1ec3m \u0111 kho\u1ea3n 2; \u0111i\u1ec3m a, \u0111i\u1ec3m d, \u0111i\u1ec3m \u0111\n",
            "kho\u1ea3n 3; kho\u1ea3n 4; \u0111i\u1ec3m a, \u0111i\u1ec3m c kho\u1ea3n 5; \u0111i\u1ec3m a, \u0111i\u1ec3m b, \u0111i\u1ec3m d, \u0111i\u1ec3m \u0111, \u0111i\u1ec3m e, \u0111i\u1ec3m i\n",
            "kho\u1ea3n 6; \u0111i\u1ec3m c, \u0111i\u1ec3m d kho\u1ea3n 7; \u0111i\u1ec3m a, \u0111i\u1ec3m b kho\u1ea3n 8; \u0111i\u1ec3m \u0111 kho\u1ea3n 9 \u0110i\u1ec1u n\u00e0y;\n",
            "b) Kh\u00f4ng b\u00e1o hi\u1ec7u b\u1eb1ng \u0111\u00e8n kh\u1ea9n c\u1ea5p ho\u1eb7c kh\u00f4ng \u0111\u1eb7t bi\u1ec3n c\u1ea3nh b\u00e1o \u201cCh\u00fa \u00fd xe \u0111\u1ed7\u201d theo\n",
            "quy \u0111\u1ecbnh trong tr\u01b0\u1eddng h\u1ee3p g\u1eb7p s\u1ef1c\u1ed1k\u1ef9thu\u1eadt (ho\u1eb7c b\u1ea5t kh\u1ea3kh\u00e1ng kh\u00e1c) bu\u1ed9c ph\u1ea3i \u0111\u1ed7xe\n",
            "chi\u1ebfm m\u1ed9t ph\u1ea7n \u0111\u01b0\u1eddng xe ch\u1ea1y ho\u1eb7c t\u1ea1i n\u01a1i kh\u00f4ng \u0111\u01b0\u1ee3c ph\u00e9p \u0111\u1ed7xe, tr\u1eebh\u00e0nh vi vi ph\u1ea1m\n",
            "quy \u0111\u1ecbnh t\u1ea1i \u0111i\u1ec3m b kho\u1ea3n 6 \u0110i\u1ec1u n\u00e0y.\n",
            "2. Ph\u1ea1t ti\u1ec1n t\u1eeb600.000 \u0111\u1ed3ng \u0111\u1ebfn 800.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c hi\u1ec7n m\u1ed9t\n",
            "trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:' metadata={'source': '/content/luatgt2.pdf', 'start_index': 47818}\n",
            "Document 3------------------------------------------------------------\n",
            "page_content='giao th\u00f4ng \u0111\u01b0\u1eddng b\u1ed9\n",
            "1. Ph\u1ea1t ti\u1ec1n t\u1eeb200.000 \u0111\u1ed3ng \u0111\u1ebfn 400.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c hi\u1ec7n m\u1ed9t\n",
            "trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) Kh\u00f4ng ch\u1ea5p h\u00e0nh hi\u1ec7u l\u1ec7nh, ch\u1ec9d\u1eabn c\u1ee7a bi\u1ec3n b\u00e1o hi\u1ec7u, v\u1ea1ch k\u1ebb\u0111\u01b0\u1eddng, tr\u1eebc\u00e1c h\u00e0nh vi\n",
            "vi ph\u1ea1m quy \u0111\u1ecbnh t\u1ea1i \u0111i\u1ec3m b, \u0111i\u1ec3m d, \u0111i\u1ec3m e kho\u1ea3n 2; \u0111i\u1ec3m a, \u0111i\u1ec3m c, \u0111i\u1ec3m d, \u0111i\u1ec3m h\n",
            "kho\u1ea3n 3; \u0111i\u1ec3m a, \u0111i\u1ec3m b, \u0111i\u1ec3m c, \u0111i\u1ec3m d kho\u1ea3n 4; \u0111i\u1ec3m b, \u0111i\u1ec3m d kho\u1ea3n 6; \u0111i\u1ec3m a, \u0111i\u1ec3m b,\n",
            "\u0111i\u1ec3m c kho\u1ea3n 7; \u0111i\u1ec3m a kho\u1ea3n 8; \u0111i\u1ec3m b kho\u1ea3n 9; \u0111i\u1ec3m a kho\u1ea3n 10 \u0110i\u1ec1u n\u00e0y;\n",
            "b) Kh\u00f4ng c\u00f3 t\u00edn hi\u1ec7u tr\u01b0\u1edbc khi v\u01b0\u1ee3t ho\u1eb7c c\u00f3 t\u00edn hi\u1ec7u v\u01b0\u1ee3t xe nh\u01b0ng kh\u00f4ng s\u1eedd\u1ee5ng trong\n",
            "su\u1ed1t qu\u00e1 tr\u00ecnh v\u01b0\u1ee3t xe;\n",
            "c) L\u00f9i xe m\u00f4 t\u00f4 ba b\u00e1nh kh\u00f4ng quan s\u00e1t hai b\u00ean, ph\u00eda sau xe ho\u1eb7c kh\u00f4ng c\u00f3 t\u00edn hi\u1ec7u l\u00f9i xe;\n",
            "d) Ch\u1edfng\u01b0\u1eddi ng\u1ed3i tr\u00ean xe s\u1eedd\u1ee5ng \u00f4 (d\u00f9);\n",
            "\u0111) Kh\u00f4ng tu\u00e2n th\u1ee7c\u00e1c quy \u0111\u1ecbnh v\u1ec1nh\u01b0\u1eddng \u0111\u01b0\u1eddng t\u1ea1i n\u01a1i \u0111\u01b0\u1eddng giao nhau, tr\u1eebc\u00e1c h\u00e0nh\n",
            "vi vi ph\u1ea1m quy \u0111\u1ecbnh t\u1ea1i \u0111i\u1ec3m c, \u0111i\u1ec3m d kho\u1ea3n 6 \u0110i\u1ec1u n\u00e0y;' metadata={'source': '/content/luatgt2.pdf', 'start_index': -1}\n",
            "Document 4------------------------------------------------------------\n",
            "page_content='ho\u1eb7c \u0111i v\u00e0o \u0111\u01b0\u1eddng c\u00f3 bi\u1ec3n b\u00e1o hi\u1ec7u c\u00f3 n\u1ed9i dung c\u1ea5m \u0111i v\u00e0o \u0111\u1ed1i v\u1edbi lo\u1ea1i ph\u01b0\u01a1ng ti\u1ec7n \u0111ang\n",
            "\u0111i\u1ec1u khi\u1ec3n g\u00e2y tai n\u1ea1n giao th\u00f4ng, tr\u1eebc\u00e1c h\u00e0nh vi vi ph\u1ea1m quy \u0111\u1ecbnh t\u1ea1i \u0111i\u1ec3m \u0111 kho\u1ea3n 11\n",
            "\u0110i\u1ec1u n\u00e0y;\n",
            "b) Vi ph\u1ea1m quy \u0111\u1ecbnh t\u1ea1i m\u1ed9t trong c\u00e1c \u0111i\u1ec3m, kho\u1ea3n sau c\u1ee7a \u0110i\u1ec1u n\u00e0y m\u00e0 g\u00e2y tai n\u1ea1n giao\n",
            "th\u00f4ng: \u0111i\u1ec3m a, \u0111i\u1ec3m b, \u0111i\u1ec3m c, \u0111i\u1ec3m d, \u0111i\u1ec3m \u0111 kho\u1ea3n 1; \u0111i\u1ec3m c kho\u1ea3n 2; \u0111i\u1ec3m b, \u0111i\u1ec3m g,\n",
            "\u0111i\u1ec3m h, \u0111i\u1ec3m n, \u0111i\u1ec3m o, \u0111i\u1ec3m p, kho\u1ea3n 3; \u0111i\u1ec3m a, \u0111i\u1ec3m c, \u0111i\u1ec3m d kho\u1ea3n 4; \u0111i\u1ec3m c, \u0111i\u1ec3m d,\n",
            "\u0111i\u1ec3m e, \u0111i\u1ec3m h, \u0111i\u1ec3m n, \u0111i\u1ec3m o, \u0111i\u1ec3m q kho\u1ea3n 5; \u0111i\u1ec3m b kho\u1ea3n 7; \u0111i\u1ec3m b, \u0111i\u1ec3m c, \u0111i\u1ec3m d\n",
            "kho\u1ea3n 9 \u0110i\u1ec1u n\u00e0y.\n",
            "11. Ph\u1ea1t ti\u1ec1n t\u1eeb30.000.000 \u0111\u1ed3ng \u0111\u1ebfn 40.000.000 \u0111\u1ed3ng \u0111\u1ed1i v\u1edbi ng\u01b0\u1eddi \u0111i\u1ec1u khi\u1ec3n xe th\u1ef1c\n",
            "hi\u1ec7n m\u1ed9t trong c\u00e1c h\u00e0nh vi vi ph\u1ea1m sau \u0111\u00e2y:\n",
            "a) \u0110i\u1ec1u khi\u1ec3n xe tr\u00ean \u0111\u01b0\u1eddng m\u00e0 trong m\u00e1u ho\u1eb7c h\u01a1i th\u1edfc\u00f3 n\u1ed3ng \u0111\u1ed9c\u1ed3n v\u01b0\u1ee3t qu\u00e1 80\n",
            "miligam/100 milil\u00edt m\u00e1u ho\u1eb7c v\u01b0\u1ee3t qu\u00e1 0,4 miligam/1 l\u00edt kh\u00ed th\u1edf;\n",
            "b) Kh\u00f4ng ch\u1ea5p h\u00e0nh y\u00eau c\u1ea7u ki\u1ec3m tra v\u1ec1n\u1ed3ng \u0111\u1ed9c\u1ed3n c\u1ee7a ng\u01b0\u1eddi thi h\u00e0nh c\u00f4ng v\u1ee5;' metadata={'source': '/content/luatgt2.pdf', 'start_index': 31313}\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mC6fJGNAqXNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgspLOvNi9dz"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}