{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfSlQyVjJ6d+wex0fXAv2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02c6313896d34a97a4817d8d477ce188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_688a441a111241e1aa1177c4c704267a",
              "IPY_MODEL_968797820c2d45f4b709214142ba8c98",
              "IPY_MODEL_9a1a2e79a42d4d1aa07b5ae3f4a11d71"
            ],
            "layout": "IPY_MODEL_8ff7b45ddcd641f7862e8c385e9eb843"
          }
        },
        "688a441a111241e1aa1177c4c704267a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6344668ba9214c96a1abe18e0582844a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_63d4935922ba4c339950a80331497c8e",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "968797820c2d45f4b709214142ba8c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_968fd8cabbe54e28a502f5f7b0e1be31",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4beb3c70d3b94ea09d1c95c2704bab89",
            "value": 8
          }
        },
        "9a1a2e79a42d4d1aa07b5ae3f4a11d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2415ff77368843faad778b137ca0e7be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_58eb3ce6e1b24eca99eca00e6df21159",
            "value": "‚Äá8/8‚Äá[01:23&lt;00:00,‚Äá‚Äá9.48s/it]"
          }
        },
        "8ff7b45ddcd641f7862e8c385e9eb843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6344668ba9214c96a1abe18e0582844a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d4935922ba4c339950a80331497c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "968fd8cabbe54e28a502f5f7b0e1be31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4beb3c70d3b94ea09d1c95c2704bab89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2415ff77368843faad778b137ca0e7be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58eb3ce6e1b24eca99eca00e6df21159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenfan20/AI-Agents-using-LangChain/blob/main/Vietnamese_Legal_Traffic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T·ªïng quan d·ª± √°n ü§ñ"
      ],
      "metadata": {
        "id": "If8uKmFJb7WP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook n√†y tr√¨nh b√†y c√°ch t·∫°o m·ªôt chatbot b·∫±ng h·ªá th·ªëng RAG d·ª±a v√†o th∆∞ vi·ªán LangChain v√† LLMs"
      ],
      "metadata": {
        "id": "C2YLtB01cSR-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6fB8o1BHxf1",
        "outputId": "ab09233c-8e1e-4e24-9c0b-4bb3e71fc68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: fsspec 2024.12.0\n",
            "Uninstalling fsspec-2024.12.0:\n",
            "  Successfully uninstalled fsspec-2024.12.0\n",
            "Found existing installation: datasets 3.5.0\n",
            "Uninstalling datasets-3.5.0:\n",
            "  Successfully uninstalled datasets-3.5.0\n",
            "Found existing installation: gcsfs 2024.12.0\n",
            "Uninstalling gcsfs-2024.12.0:\n",
            "  Successfully uninstalled gcsfs-2024.12.0\n",
            "Collecting fsspec==2024.12.0\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets==3.5.0\n",
            "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting gcsfs==2024.12.0\n",
            "  Using cached gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.5.0) (6.0.2)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs==2024.12.0) (2.19.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.5.0) (1.18.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs==2024.12.0) (4.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.5.0) (2025.1.31)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs==2024.12.0) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs==2024.12.0) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.5.0) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs==2024.12.0) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2024.12.0) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.5.0) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2024.12.0) (3.2.2)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "Using cached gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: fsspec, datasets, gcsfs\n",
            "Successfully installed datasets-3.5.0 fsspec-2024.12.0 gcsfs-2024.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y fsspec datasets gcsfs\n",
        "!pip install fsspec==2024.12.0 datasets==3.5.0 gcsfs==2024.12.0\n",
        "!pip install -q torch transformers accelerate bitsandbytes \\\n",
        "  langchain sentence-transformers faiss-cpu openpyxl pacmap datasets \\\n",
        "  langchain-community ragatouille tqdm pymupdf python-docx pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "FILE_01 = '/content/luatgt2.pdf'\n",
        "\n",
        "\n",
        "VECTOR_DATABASE_PATH = '/content/vectordatabase'\n",
        "os.makedirs('/content/vectordatabase', exist_ok=True)"
      ],
      "metadata": {
        "id": "tDm7mOE_H7Xh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking üî™"
      ],
      "metadata": {
        "id": "tRbi5bd0sjjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from docx import Document as DocxDocument\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_pdf_file(file_path):\n",
        "    \"\"\"Loads a PDF file and returns its entire content using PyMuPDFLoader.\"\"\"\n",
        "    loader = PyMuPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    full_content = \"\"\n",
        "    for doc in documents:\n",
        "        full_content += doc.page_content + \"\\n\"  # Add a newline to separate pages\n",
        "    return full_content  # Return the entire content\n",
        "\n",
        "def load_txt_file(file_path):\n",
        "    \"\"\"Loads a TXT file and returns its content as a string.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def load_docx_file(file_path):\n",
        "    \"\"\"Loads a DOCX file and returns its content as a string.\"\"\"\n",
        "    doc = DocxDocument(file_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return \"\\n\".join(full_text)\n",
        "\n",
        "def load_csv_file(file_path):\n",
        "    \"\"\"Loads a CSV file and concatenates all rows as a single string.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.to_string(index=False)  # Converts the DataFrame to a string (without row indices)\n",
        "\n",
        "def load_file(file_path):\n",
        "    \"\"\"Determines the file type and loads the file content.\"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == '.txt':\n",
        "        return load_txt_file(file_path)\n",
        "    elif ext in ['.doc', '.docx']:\n",
        "        return load_docx_file(file_path)\n",
        "    elif ext == '.pdf':\n",
        "        return load_pdf_file(file_path)\n",
        "    elif ext == '.csv':\n",
        "        return load_csv_file(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")"
      ],
      "metadata": {
        "id": "A1B1p27JLJ1P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of document file paths (PDF, DOCX, TXT, CSV)\n",
        "file_paths = [FILE_01] # Example file paths. It can be like: file_paths = [FILE_01, FILE_02,..]\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = []\n",
        "for file_path in tqdm(file_paths):\n",
        "    try:\n",
        "        content = load_file(file_path)\n",
        "        RAW_KNOWLEDGE_BASE.append(\n",
        "            LangchainDocument(page_content=content, metadata={\"source\": file_path})\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {file_path}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzuAON3ELhlo",
        "outputId": "6c0f97e4-af3a-462f-8729-cf49aa017131"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    print(f\"Source: {doc.metadata['source']}\")\n",
        "    print(f\"Content snippet: {doc.page_content[:500]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mOxisvoLnZP",
        "outputId": "11ca4edf-4fe9-429f-a07c-3a7f660b0aaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: /content/luatgt2.pdf\n",
            "Content snippet: CH√çNH PH·ª¶\n",
            "--------\n",
            "C·ªòNG H√íA X√É H·ªòI CH·ª¶NGHƒ®A VI·ªÜT NAM\n",
            "ƒê·ªôc l·∫≠p - T·ª±do - H·∫°nh ph√∫c\n",
            "---------------\n",
            "S·ªë: 168/2024/Nƒê-CP\n",
            "H√† N·ªôi, ng√†y 26 th√°ng 12 nƒÉm 2024\n",
            "NGH·ªäƒê·ªäNH\n",
            "QUY ƒê·ªäNH X·ª¨PH·∫†T VI PH·∫†M H√ÄNH CH√çNH V·ªÄTR·∫¨T T·ª∞, AN TO√ÄN GIAO\n",
            "TH√îNG TRONG Lƒ®NH V·ª∞C GIAO TH√îNG ƒê∆Ø·ªúNG B·ªò; TR·ª™ƒêI·ªÇM, PH·ª§C H·ªíI\n",
            "ƒêI·ªÇM GI·∫§Y PH√âP L√ÅI XE\n",
            "CƒÉn c·ª©Lu·∫≠t T·ªïch·ª©c Ch√≠nh ph·ªßng√†y 19 th√°ng 6 nƒÉm 2015; Lu·∫≠t s·ª≠a ƒë·ªïi, b·ªïsung m·ªôt\n",
            "s·ªëƒëi·ªÅu c·ªßa Lu·∫≠t T·ªïch·ª©c Ch√≠nh ph·ªßv√† Lu·∫≠t T·ªïch·ª©c ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng ng√†y 22\n",
            "th√°ng 11 nƒÉm 2019;\n",
            "CƒÉn c·ª©Lu·∫≠t X·ª≠l√Ω vi ph·∫°m h√†...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "    chunk_overlap=100,  # The number of characters to overlap between chunks\n",
        "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ],
      "metadata": {
        "id": "XU34rNVHLrDP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\n",
        "print(\n",
        "    f\"Model's maximum sequence length: {SentenceTransformer('minhtt/phobert-base-v2').max_seq_length}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palqBVZRLw06",
        "outputId": "1c0aa37a-8c59-40d4-e07e-208956bf437b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name minhtt/phobert-base-v2. Creating a new one with mean pooling.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at minhtt/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's maximum sequence length: 258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding document üìÇ"
      ],
      "metadata": {
        "id": "SXsfngHhuDnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"AITeamVN/Vietnamese_Embedding\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    258,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")"
      ],
      "metadata": {
        "id": "h9SqYZjLL1fl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database üìÇ"
      ],
      "metadata": {
        "id": "KJ8b7xebvXrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2sMpEQtMKzL",
        "outputId": "7de18eb1-9ace-4241-e70a-20055ea995a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-26065c6d9089>:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L∆∞u Vector Database ƒë∆∞·ª£c x·ª≠ l√Ω v√†o c√°c th∆∞ m·ª•c"
      ],
      "metadata": {
        "id": "EuGgwbRvwgeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss  # Make sure to import FAISS\n",
        "import pickle\n",
        "\n",
        "# Save the FAISS index to a file\n",
        "faiss_file_path = os.path.join(VECTOR_DATABASE_PATH, 'faiss_index.bin')\n",
        "faiss.write_index(KNOWLEDGE_VECTOR_DATABASE.index, faiss_file_path)\n",
        "print(f\"FAISS index saved to {faiss_file_path}\")\n",
        "\n",
        "# Save the document store to a pickle file\n",
        "docstore_file_path = os.path.join(VECTOR_DATABASE_PATH, 'docstore.pkl')\n",
        "with open(docstore_file_path, 'wb') as f:\n",
        "    pickle.dump(KNOWLEDGE_VECTOR_DATABASE.docstore, f)\n",
        "print(f\"Document store saved to {docstore_file_path}\")\n",
        "\n",
        "# Save the index_to_docstore_id mapping\n",
        "mapping_file_path = os.path.join(VECTOR_DATABASE_PATH, 'index_to_docstore_id.pkl')\n",
        "with open(mapping_file_path, 'wb') as f:\n",
        "    pickle.dump(KNOWLEDGE_VECTOR_DATABASE.index_to_docstore_id, f)\n",
        "print(f\"Index to document store ID mapping saved to {mapping_file_path}\")\n",
        "\n",
        "# Load the FAISS index from the file\n",
        "loaded_index = faiss.read_index(faiss_file_path)\n",
        "\n",
        "# Load the document store from the pickle file\n",
        "with open(docstore_file_path, 'rb') as f:\n",
        "    loaded_docstore = pickle.load(f)\n",
        "\n",
        "# Load the index_to_docstore_id mapping\n",
        "with open(mapping_file_path, 'rb') as f:\n",
        "    index_to_docstore_id = pickle.load(f)\n",
        "\n",
        "# Create the FAISS vector store using the loaded index and document store\n",
        "loaded_vector_database = FAISS(\n",
        "    index=loaded_index,\n",
        "    docstore=loaded_docstore,\n",
        "    index_to_docstore_id=index_to_docstore_id,\n",
        "    embedding_function=embedding_model.embed_query\n",
        ")\n",
        "print(\"Vector database loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehKDq37DMfrt",
        "outputId": "de73b0ce-684b-4d82-c5fc-e65d7937014b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index saved to /content/vectordatabase/faiss_index.bin\n",
            "Document store saved to /content/vectordatabase/docstore.pkl\n",
            "Index to document store ID mapping saved to /content/vectordatabase/index_to_docstore_id.pkl\n",
            "Vector database loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain v·ªõi th∆∞ m·ª•c ƒë√£ l∆∞u"
      ],
      "metadata": {
        "id": "qcRAMGk7wwjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed a user query in the same space\n",
        "user_query = \"Quy ƒë·ªãnh v·ªÅ ƒë·ªô tu·ªïi l√°i xe m√¥ t√¥ hai b√°nh?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ],
      "metadata": {
        "id": "x72vDcblMr4w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "# retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "\n",
        "retrieved_docs = loaded_vector_database.similarity_search(query=user_query, k=5)\n",
        "\n",
        "print(\n",
        "    \"\\n==================================Top document==================================\"\n",
        ")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tVUVC5_M6B6",
        "outputId": "12447393-fe9e-4f65-ba1f-cfb2713be60d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting retrieval for user_query='Quy ƒë·ªãnh v·ªÅ ƒë·ªô tu·ªïi l√°i xe m√¥ t√¥ hai b√°nh?'...\n",
            "\n",
            "==================================Top document==================================\n",
            "xe).\n",
            "6. Ph·∫°t ti·ªÅn t·ª´4.000.000 ƒë·ªìng ƒë·∫øn 6.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi t·ª´ƒë·ªß16 tu·ªïi ƒë·∫øn d∆∞·ªõi\n",
            "18 tu·ªïi ƒëi·ªÅu khi·ªÉn xe √¥ t√¥, xe ch·ªüng∆∞·ªùi b·ªën b√°nh c√≥ g·∫Øn ƒë·ªông c∆°, xe ch·ªüh√†ng b·ªën b√°nh\n",
            "c√≥ g·∫Øn ƒë·ªông c∆° v√† c√°c lo·∫°i xe t∆∞∆°ng t·ª±xe √¥ t√¥.\n",
            "7. Ph·∫°t ti·ªÅn t·ª´6.000.000 ƒë·ªìng ƒë·∫øn 8.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe m√¥ t√¥ hai\n",
            "b√°nh c√≥ dung t√≠ch xi-lanh tr√™n 125 cm3 tr·ªül√™n ho·∫∑c c√≥ c√¥ng su·∫•t ƒë·ªông c∆° ƒëi·ªán tr√™n 11\n",
            "kW, xe m√¥ t√¥ ba b√°nh th·ª±c hi·ªán m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) C√≥ gi·∫•y ph√©p l√°i xe nh∆∞ng kh√¥ng ph√π h·ª£p v·ªõi lo·∫°i xe ƒëang ƒëi·ªÅu khi·ªÉn;\n",
            "b) Kh√¥ng c√≥ gi·∫•y ph√©p l√°i xe ho·∫∑c s·ª≠d·ª•ng gi·∫•y ph√©p l√°i xe ƒë√£ b·ªãtr·ª´h·∫øt ƒëi·ªÉm, gi·∫•y ph√©p\n",
            "l√°i xe kh√¥ng do c∆° quan c√≥ th·∫©m quy·ªÅn c·∫•p, gi·∫•y ph√©p l√°i xe b·ªãt·∫©y x√≥a, gi·∫•y ph√©p l√°i xe\n",
            "kh√¥ng c√≤n hi·ªáu l·ª±c;\n",
            "==================================Metadata==================================\n",
            "{'source': '/content/luatgt2.pdf', 'start_index': 99836}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ƒê∆∞a LLM ƒë·ªÉ t·∫°o sinh c√¢u tr·∫£ l·ªùi v·ªõi ƒë·ªØ li·ªáu ƒë∆∞·ª£c chain"
      ],
      "metadata": {
        "id": "cEvU27LIxcTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME, quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "02c6313896d34a97a4817d8d477ce188",
            "688a441a111241e1aa1177c4c704267a",
            "968797820c2d45f4b709214142ba8c98",
            "9a1a2e79a42d4d1aa07b5ae3f4a11d71",
            "8ff7b45ddcd641f7862e8c385e9eb843",
            "6344668ba9214c96a1abe18e0582844a",
            "63d4935922ba4c339950a80331497c8e",
            "968fd8cabbe54e28a502f5f7b0e1be31",
            "4beb3c70d3b94ea09d1c95c2704bab89",
            "2415ff77368843faad778b137ca0e7be",
            "58eb3ce6e1b24eca99eca00e6df21159"
          ]
        },
        "id": "ZIju0plfNAjW",
        "outputId": "6f0029fb-246d-4a11-fe0f-c6c4f1db4170"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02c6313896d34a97a4817d8d477ce188"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langdetect"
      ],
      "metadata": {
        "id": "8du24nceNRgB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "\n",
        "prompt_in_chat_format_en = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prompt template for Vietnamese\n",
        "prompt_in_chat_format_vi = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"S·ª≠ d·ª•ng th√¥ng tin trong ng·ªØ c·∫£nh, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß cho c√¢u h·ªèi.\n",
        "Ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi, c√¢u tr·∫£ l·ªùi c·∫ßn ng·∫Øn g·ªçn v√† ph√π h·ª£p v·ªõi c√¢u h·ªèi.\n",
        "Cung c·∫•p s·ªë c·ªßa t√†i li·ªáu ngu·ªìn khi ph√π h·ª£p.\n",
        "N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ suy ra t·ª´ ng·ªØ c·∫£nh, kh√¥ng ƒë∆∞a ra c√¢u tr·∫£ l·ªùi.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Ng·ªØ c·∫£nh:\n",
        "{context}\n",
        "---\n",
        "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi m√† b·∫°n c·∫ßn tr·∫£ l·ªùi.\n",
        "\n",
        "C√¢u h·ªèi: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "def detect_language(query):\n",
        "    return detect(query)\n",
        "\n",
        "def create_prompt(question):\n",
        "\n",
        "    language = detect_language(question)\n",
        "\n",
        "    if language == 'vi':\n",
        "        RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "            prompt_in_chat_format_vi, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    else:\n",
        "        RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "            prompt_in_chat_format_en, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "    return RAG_PROMPT_TEMPLATE\n",
        "\n",
        "#test prompt if the question is Vietnamese\n",
        "\n",
        "test_prompt = create_prompt(\"H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa n∆∞·ªõc n√†o?\")\n",
        "print(test_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-XNwX-iOBcP",
        "outputId": "cc42018b-9f80-40f0-b15c-2fd4b2015cbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "S·ª≠ d·ª•ng th√¥ng tin trong ng·ªØ c·∫£nh, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß cho c√¢u h·ªèi.\n",
            "Ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi ƒë∆∞·ª£c h·ªèi, c√¢u tr·∫£ l·ªùi c·∫ßn ng·∫Øn g·ªçn v√† ph√π h·ª£p v·ªõi c√¢u h·ªèi.\n",
            "Cung c·∫•p s·ªë c·ªßa t√†i li·ªáu ngu·ªìn khi ph√π h·ª£p.\n",
            "N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ suy ra t·ª´ ng·ªØ c·∫£nh, kh√¥ng ƒë∆∞a ra c√¢u tr·∫£ l·ªùi.</s>\n",
            "<|user|>\n",
            "Ng·ªØ c·∫£nh:\n",
            "{context}\n",
            "---\n",
            "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi m√† b·∫°n c·∫ßn tr·∫£ l·ªùi.\n",
            "\n",
            "C√¢u h·ªèi: {question}</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs_text = [\n",
        "    doc.page_content for doc in retrieved_docs\n",
        "]  # We only need the text of the documents\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join(\n",
        "    [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)]\n",
        ")\n",
        "\n",
        "user_query=\"Quy ƒë·ªãnh v·ªÅ ƒë·ªô tu·ªïi l√°i xe m√¥ t√¥ 2 b√°nh?\"\n",
        "\n",
        "rag_prompt = create_prompt(user_query)\n",
        "final_prompt = rag_prompt.format(\n",
        "    question=user_query, context=context\n",
        ")\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhkV9GeSOMxi",
        "outputId": "3130f863-8360-4e39-afc6-94cd7162c0d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theo quy ƒë·ªãnh, ng∆∞·ªùi l√°i xe m√¥ t√¥ hai b√°nh ph·∫£i ƒë∆∞·ª£c tu·ªïi 16 ho·∫∑c tr·ªü l√™n. (ƒêi·ªÅu 18, Document 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Pipeline\n",
        "from typing import Tuple, List\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "from faiss import Index as FAISS\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "\n",
        "    # Ensure that relevant_docs is not empty or None\n",
        "    if not relevant_docs:\n",
        "        raise ValueError(\"No relevant documents retrieved.\")\n",
        "\n",
        "    # Keep only the text from the retrieved documents\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    # Ensure k is not larger than the number of documents\n",
        "    num_docs_final = min(num_docs_final, len(relevant_docs))\n",
        "\n",
        "    if num_docs_final < 1:\n",
        "        raise ValueError(\"Not enough documents for processing.\")\n",
        "\n",
        "    # Limit to the final number of documents\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    RAG_PROMPT_TEMPLATE = create_prompt(question)\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Generate the answer using the LLM\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "metadata": {
        "id": "CqoSJDAEORYK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Quy ƒë·ªãnh v·ªÅ m·ª©c ph·∫°t v·ªõi vi ph·∫°m n·ªìng ƒë·ªô c·ªìn trong m√°u?\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(\n",
        "    question, READER_LLM, loaded_vector_database)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HBvtBwSP-gd",
        "outputId": "58cd2cb0-9e78-40ce-cae9-02c8184072fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Retrieving documents...\n",
            "=> Generating answer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaj6DxnGQi4_",
        "outputId": "f80f443a-a836-4bde-e782-2e6abbb0e9b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================Answer==================================\n",
            "Quy ƒë·ªãnh v·ªÅ m·ª©c ph·∫°t v·ªõi vi ph·∫°m n·ªìng ƒë·ªôc·ªìn trong m√°u ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh trong c√°c tr∆∞·ªùng h·ª£p sau:\n",
            "\n",
            "- Document 1: N·ªìng ƒë·ªôc·ªìn nh∆∞ng ch∆∞a v∆∞·ª£t qu√° 50 miligam/100 milil√≠t m√°u ho·∫∑c ch∆∞a v∆∞·ª£t qu√° 0,25 miligam/1 l√≠t kh√≠ th·ªü (ƒêi·ªÅu kh√°u a trong c√°c h√†nh vi vi ph·∫°m): Ph·∫°t ti·ªÅn t·ª´ 12.000.000 ƒë·ªìng ƒë·∫øn 14.000.000 ƒë·ªìng.\n",
            "\n",
            "- Document 2: N·ªìng ƒë·ªôc·ªìn v∆∞·ª£t qu√° 80 miligam/100 milil√≠t m√°u (ƒêi·ªÅu kh√°u a trong c√°c h√†nh vi vi ph·∫°m): Ph·∫°t ti·ªÅn t·ª´ 18.000.000 ƒë·ªìng ƒë·∫øn 20.000.000 ƒë·ªìng.\n",
            "\n",
            "- Document 3: Khi c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn v·ª• ta n·∫°n giao th√¥ng, kh√¥ng d·ª´ng ngay ph∆∞∆°ng ti·ªán, kh√¥ng gi·ªØnguy√™n hi·ªán tr∆∞·ªùng, kh√¥ng tr·ª£gi√∫p ng∆∞·ªùi b·ªã n·∫°n (ƒêi·ªÅu kh√°u a trong c√°c h√†nh vi vi ph·∫°m): Ph·∫°t ti·ªÅn theo quy ƒë·ªãnh t·∫°i ƒëi·ªÉm c kho·∫£n 9 ƒêi·ªÅu n√†y.\n",
            "\n",
            "C√°c quy ƒë·ªãnh tr√™n c√≥ hi·ªáu l·ª±c ch√≠nh quy·ªÅn n·∫øu ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe\n",
            "==================================Source docs==================================\n",
            "Document 0------------------------------------------------------------\n",
            "ho·∫∑c ƒëi v√†o ƒë∆∞·ªùng c√≥ bi·ªÉn b√°o hi·ªáu c√≥ n·ªôi dung c·∫•m ƒëi v√†o ƒë·ªëi v·ªõi lo·∫°i ph∆∞∆°ng ti·ªán ƒëang\n",
            "ƒëi·ªÅu khi·ªÉn g√¢y tai n·∫°n giao th√¥ng, tr·ª´c√°c h√†nh vi vi ph·∫°m quy ƒë·ªãnh t·∫°i ƒëi·ªÉm ƒë kho·∫£n 11\n",
            "ƒêi·ªÅu n√†y;\n",
            "b) Vi ph·∫°m quy ƒë·ªãnh t·∫°i m·ªôt trong c√°c ƒëi·ªÉm, kho·∫£n sau c·ªßa ƒêi·ªÅu n√†y m√† g√¢y tai n·∫°n giao\n",
            "th√¥ng: ƒëi·ªÉm a, ƒëi·ªÉm b, ƒëi·ªÉm c, ƒëi·ªÉm d, ƒëi·ªÉm ƒë kho·∫£n 1; ƒëi·ªÉm c kho·∫£n 2; ƒëi·ªÉm b, ƒëi·ªÉm g,\n",
            "ƒëi·ªÉm h, ƒëi·ªÉm n, ƒëi·ªÉm o, ƒëi·ªÉm p, kho·∫£n 3; ƒëi·ªÉm a, ƒëi·ªÉm c, ƒëi·ªÉm d kho·∫£n 4; ƒëi·ªÉm c, ƒëi·ªÉm d,\n",
            "ƒëi·ªÉm e, ƒëi·ªÉm h, ƒëi·ªÉm n, ƒëi·ªÉm o, ƒëi·ªÉm q kho·∫£n 5; ƒëi·ªÉm b kho·∫£n 7; ƒëi·ªÉm b, ƒëi·ªÉm c, ƒëi·ªÉm d\n",
            "kho·∫£n 9 ƒêi·ªÅu n√†y.\n",
            "11. Ph·∫°t ti·ªÅn t·ª´30.000.000 ƒë·ªìng ƒë·∫øn 40.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c\n",
            "hi·ªán m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) ƒêi·ªÅu khi·ªÉn xe tr√™n ƒë∆∞·ªùng m√† trong m√°u ho·∫∑c h∆°i th·ªüc√≥ n·ªìng ƒë·ªôc·ªìn v∆∞·ª£t qu√° 80\n",
            "Document 1------------------------------------------------------------\n",
            "a) ƒêi·ªÅu khi·ªÉn xe ch·∫°y qu√° t·ªëc ƒë·ªôquy ƒë·ªãnh tr√™n 20 km/h ƒë·∫øn 35 km/h;\n",
            "b) Kh√¥ng nh∆∞·ªùng ƒë∆∞·ªùng ho·∫∑c g√¢y c·∫£n tr·ªüxe ƒë∆∞·ª£c quy·ªÅn ∆∞u ti√™n ƒëang ph√°t t√≠n hi·ªáu ∆∞u\n",
            "ti√™n ƒëi l√†m nhi·ªám v·ª•;\n",
            "c) ƒêi·ªÅu khi·ªÉn xe tr√™n ƒë∆∞·ªùng m√† trong m√°u ho·∫∑c h∆°i th·ªüc√≥ n·ªìng ƒë·ªôc·ªìn nh∆∞ng ch∆∞a v∆∞·ª£t\n",
            "qu√° 50 miligam/100 milil√≠t m√°u ho·∫∑c ch∆∞a v∆∞·ª£t qu√° 0,25 miligam/1 l√≠t kh√≠ th·ªü;\n",
            "d) ƒêi·ªÅu khi·ªÉn xe ƒëi tr√™n v·ªâa h√®, tr·ª´tr∆∞·ªùng h·ª£p ƒëi·ªÅu khi·ªÉn xe ƒëi qua v·ªâa h√® ƒë·ªÉv√†o nh√†, c∆°\n",
            "quan.\n",
            "7. Ph·∫°t ti·ªÅn t·ª´12.000.000 ƒë·ªìng ƒë·∫øn 14.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c\n",
            "hi·ªán m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) ƒêi·ªÅu khi·ªÉn xe ch·∫°y qu√° t·ªëc ƒë·ªôquy ƒë·ªãnh tr√™n 35 km/h;\n",
            "b) ƒêi·ªÅu khi·ªÉn xe ch·ªüng∆∞·ªùi b·ªën b√°nh c√≥ g·∫Øn ƒë·ªông c∆°, xe ch·ªüh√†ng b·ªën b√°nh c√≥ g·∫Øn ƒë·ªông\n",
            "c∆° ƒëi v√†o ƒë∆∞·ªùng cao t·ªëc;\n",
            "Document 2------------------------------------------------------------\n",
            "c∆° quan c√¥ng an, ·ª¶y ban nh√¢n d√¢n n∆°i g·∫ßn nh·∫•t;\n",
            "d) Vi ph·∫°m quy ƒë·ªãnh t·∫°i m·ªôt trong c√°c ƒëi·ªÉm, kho·∫£n sau c·ªßa ƒêi·ªÅu n√†y m√† g√¢y tai n·∫°n giao\n",
            "th√¥ng: ƒëi·ªÉm a, ƒëi·ªÉm b kho·∫£n 1; ƒëi·ªÉm d kho·∫£n 2; ƒëi·ªÉm b, ƒëi·ªÉm c, ƒëi·ªÉm d, ƒëi·ªÉm ƒë kho·∫£n 3;\n",
            "ƒëi·ªÉm b kho·∫£n 4; ƒëi·ªÉm b kho·∫£n 5; ƒëi·ªÉm e, ƒëi·ªÉm g, ƒëi·ªÉm i kho·∫£n 6; ƒëi·ªÉm b, ƒëi·ªÉm c kho·∫£n 7\n",
            "ƒêi·ªÅu n√†y.\n",
            "9. Ph·∫°t ti·ªÅn t·ª´18.000.000 ƒë·ªìng ƒë·∫øn 20.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c\n",
            "hi·ªán m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) ƒêi·ªÅu khi·ªÉn xe tr√™n ƒë∆∞·ªùng m√† trong m√°u ho·∫∑c h∆°i th·ªüc√≥ n·ªìng ƒë·ªôc·ªìn v∆∞·ª£t qu√° 80\n",
            "miligam/100 milil√≠t m√°u ho·∫∑c v∆∞·ª£t qu√° 0,4 miligam/1 l√≠t kh√≠ th·ªü;\n",
            "b) Kh√¥ng ch·∫•p h√†nh y√™u c·∫ßu ki·ªÉm tra v·ªÅn·ªìng ƒë·ªôc·ªìn c·ªßa ng∆∞·ªùi thi h√†nh c√¥ng v·ª•;\n",
            "c) ƒêi·ªÅu khi·ªÉn xe tr√™n ƒë∆∞·ªùng m√† trong c∆° th·ªÉc√≥ ch·∫•t ma t√∫y ho·∫∑c ch·∫•t k√≠ch th√≠ch kh√°c m√†\n",
            "Document 3------------------------------------------------------------\n",
            "a) ƒêi·ªÅu khi·ªÉn xe c√≥ li√™n quan tr·ª±c ti·∫øp ƒë·∫øn v·ª•tai n·∫°n giao th√¥ng m√† kh√¥ng d·ª´ng ngay\n",
            "ph∆∞∆°ng ti·ªán, kh√¥ng gi·ªØnguy√™n hi·ªán tr∆∞·ªùng, kh√¥ng tr·ª£gi√∫p ng∆∞·ªùi b·ªãn·∫°n, tr·ª´h√†nh vi vi\n",
            "ph·∫°m quy ƒë·ªãnh t·∫°i ƒëi·ªÉm c kho·∫£n 9 ƒêi·ªÅu n√†y;\n",
            "b) Chuy·ªÉn h∆∞·ªõng kh√¥ng nh∆∞·ªùng quy·ªÅn ƒëi tr∆∞·ªõc cho: ng∆∞·ªùi ƒëi b·ªô, xe lƒÉn c·ªßa ng∆∞·ªùi\n",
            "khuy·∫øt t·∫≠t qua ƒë∆∞·ªùng t·∫°i n∆°i c√≥ v·∫°ch k·∫ªƒë∆∞·ªùng d√†nh cho ng∆∞·ªùi ƒëi b·ªô; xe th√¥ s∆° ƒëang ƒëi\n",
            "tr√™n ph·∫ßn ƒë∆∞·ªùng d√†nh cho xe th√¥ s∆°;\n",
            "c) Chuy·ªÉn h∆∞·ªõng kh√¥ng nh∆∞·ªùng ƒë∆∞·ªùng cho: c√°c xe ƒëi ng∆∞·ª£c chi·ªÅu; ng∆∞·ªùi ƒëi b·ªô, xe th√¥\n",
            "s∆° ƒëang qua ƒë∆∞·ªùng t·∫°i n∆°i kh√¥ng c√≥ v·∫°ch k·∫ªƒë∆∞·ªùng cho ng∆∞·ªùi ƒëi b·ªô.\n",
            "6. Ph·∫°t ti·ªÅn t·ª´2.000.000 ƒë·ªìng ƒë·∫øn 3.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c hi·ªán\n",
            "m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) ƒêi·ªÅu khi·ªÉn xe tr√™n ƒë∆∞·ªùng m√† trong m√°u ho·∫∑c h∆°i th·ªüc√≥ n·ªìng ƒë·ªôc·ªìn nh∆∞ng ch∆∞a v∆∞·ª£t\n",
            "Document 4------------------------------------------------------------\n",
            "ki·ªÉm so√°t giao th√¥ng;\n",
            "ƒë) Kh√¥ng nh∆∞·ªùng ƒë∆∞·ªùng ho·∫∑c g√¢y c·∫£n tr·ªüxe ƒë∆∞·ª£c quy·ªÅn ∆∞u ti√™n ƒëang ph√°t t√≠n hi·ªáu ∆∞u\n",
            "ti√™n ƒëi l√†m nhi·ªám v·ª•.\n",
            "8. Ph·∫°t ti·ªÅn t·ª´6.000.000 ƒë·ªìng ƒë·∫øn 8.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c hi·ªán\n",
            "m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) ƒêi·ªÅu khi·ªÉn xe ch·∫°y qu√° t·ªëc ƒë·ªôquy ƒë·ªãnh tr√™n 20 km/h;\n",
            "b) ƒêi·ªÅu khi·ªÉn xe tr√™n ƒë∆∞·ªùng m√† trong m√°u ho·∫∑c h∆°i th·ªüc√≥ n·ªìng ƒë·ªôc·ªìn v∆∞·ª£t qu√° 50\n",
            "miligam ƒë·∫øn 80 miligam/100 milil√≠t m√°u ho·∫∑c v∆∞·ª£t qu√° 0,25 miligam ƒë·∫øn 0,4 miligam/1\n",
            "l√≠t kh√≠ th·ªü.\n",
            "9. Ph·∫°t ti·ªÅn t·ª´8.000.000 ƒë·ªìng ƒë·∫øn 10.000.000 ƒë·ªìng ƒë·ªëi v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe th·ª±c\n",
            "hi·ªán m·ªôt trong c√°c h√†nh vi vi ph·∫°m sau ƒë√¢y:\n",
            "a) ƒêi·ªÅu khi·ªÉn xe l·∫°ng l√°ch, ƒë√°nh v√µng tr√™n ƒë∆∞·ªùng b·ªô; s·ª≠d·ª•ng ch√¢n ch·ªëng ho·∫∑c v·∫≠t kh√°c\n",
            "qu·ªát xu·ªëng ƒë∆∞·ªùng khi xe ƒëang ch·∫°y;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "h88W3dqXTVPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}